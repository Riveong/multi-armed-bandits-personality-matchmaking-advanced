{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8055577,"sourceType":"datasetVersion","datasetId":4751114}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-21T20:06:31.189632Z","iopub.execute_input":"2024-04-21T20:06:31.190126Z","iopub.status.idle":"2024-04-21T20:06:31.200539Z","shell.execute_reply.started":"2024-04-21T20:06:31.190093Z","shell.execute_reply":"2024-04-21T20:06:31.199053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![Multi Armed](https://media.discordapp.net/attachments/1023598916857499680/1228695121919344700/Desktop_-_4.png?ex=662cfa81&is=661a8581&hm=d0a7cd731cb0afdc3d67238dd26e876fac0f0e39a2b82ebe98b71f5e12630f96&=&format=webp&quality=lossless&width=1440&height=442)","metadata":{}},{"cell_type":"markdown","source":"Dataset link :<br>\nCode by :","metadata":{}},{"cell_type":"markdown","source":"# Data Overview","metadata":{}},{"cell_type":"code","source":"predf = pd.read_csv('/kaggle/input/preprocessed/preprocessed_data.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:31.202564Z","iopub.execute_input":"2024-04-21T20:06:31.202919Z","iopub.status.idle":"2024-04-21T20:06:31.826798Z","shell.execute_reply.started":"2024-04-21T20:06:31.202891Z","shell.execute_reply":"2024-04-21T20:06:31.825390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predf","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:31.828676Z","iopub.execute_input":"2024-04-21T20:06:31.829015Z","iopub.status.idle":"2024-04-21T20:06:31.845570Z","shell.execute_reply.started":"2024-04-21T20:06:31.828988Z","shell.execute_reply":"2024-04-21T20:06:31.844163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predf.isnull().values.any()","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:31.847227Z","iopub.execute_input":"2024-04-21T20:06:31.847683Z","iopub.status.idle":"2024-04-21T20:06:31.860853Z","shell.execute_reply.started":"2024-04-21T20:06:31.847641Z","shell.execute_reply":"2024-04-21T20:06:31.859504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predf=predf.drop(['Unnamed: 0'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:31.864839Z","iopub.execute_input":"2024-04-21T20:06:31.865384Z","iopub.status.idle":"2024-04-21T20:06:31.879815Z","shell.execute_reply.started":"2024-04-21T20:06:31.865340Z","shell.execute_reply":"2024-04-21T20:06:31.878422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predf","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:31.881170Z","iopub.execute_input":"2024-04-21T20:06:31.881946Z","iopub.status.idle":"2024-04-21T20:06:31.898936Z","shell.execute_reply.started":"2024-04-21T20:06:31.881893Z","shell.execute_reply":"2024-04-21T20:06:31.897403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:31.900412Z","iopub.execute_input":"2024-04-21T20:06:31.900887Z","iopub.status.idle":"2024-04-21T20:06:31.908198Z","shell.execute_reply.started":"2024-04-21T20:06:31.900846Z","shell.execute_reply":"2024-04-21T20:06:31.906583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Recommender\nLearning Policy:  \n- Greedy (epsilon)\n- Linear Greedy\n- Linear Greedy (with epsilon)\n- Thompson Sampling\n\nUsage Guide:  \nnum arm = num of items  \nnum rounds = num of iteration","metadata":{}},{"cell_type":"markdown","source":"## Greedy\n","metadata":{}},{"cell_type":"code","source":"def epsilon_greedy_bandit(dataset, num_arms, epsilon, num_rounds):\n    estimated_rewards = [0] * num_arms\n    num_selected = [0] * num_arms\n    total_rewards = 0\n    exploration_count = 0\n    exploitation_count = 0\n\n    for _ in range(num_rounds):\n        if random.random() < epsilon:\n            # Explore: Choose a random arm\n            chosen_arm = random.randint(0, num_arms - 1)\n            exploration_count += 1\n        else:\n            # Exploit: Choose the arm with the highest estimated reward\n            chosen_arm = max(range(num_arms), key=lambda arm: estimated_rewards[arm])\n            exploitation_count += 1\n\n        # Randomly select an observation for the chosen arm from the dataset\n        chosen_row = dataset[dataset['match_id'] == chosen_arm].sample(n=1)\n        \n        # Observe the reward (response) for the chosen arm\n        reward = chosen_row['response'].values[0]\n        total_rewards += reward\n\n        # Update the estimated reward for the chosen arm\n        num_selected[chosen_arm] += 1\n        estimated_rewards[chosen_arm] += (reward - estimated_rewards[chosen_arm]) / num_selected[chosen_arm]\n\n    return estimated_rewards, num_selected, exploration_count, exploitation_count, total_rewards","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:31.910267Z","iopub.execute_input":"2024-04-21T20:06:31.910809Z","iopub.status.idle":"2024-04-21T20:06:31.922950Z","shell.execute_reply.started":"2024-04-21T20:06:31.910772Z","shell.execute_reply":"2024-04-21T20:06:31.921178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = epsilon_greedy_bandit(predf, num_arms=100, epsilon=0.5, num_rounds=100)\nprint(\"Estimated rewards for each arm:\", results[0])\nprint(\"Number of times each arm was selected:\", results[1])\nprint(\"Exploration count:\", results[2])\nprint(\"Exploitation count:\", results[3])\nprint(\"Total rewards:\", results[4])","metadata":{"_kg_hide-input":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-04-21T20:06:31.925328Z","iopub.execute_input":"2024-04-21T20:06:31.925864Z","iopub.status.idle":"2024-04-21T20:06:32.080731Z","shell.execute_reply.started":"2024-04-21T20:06:31.925821Z","shell.execute_reply":"2024-04-21T20:06:32.079183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Thompson Sampling","metadata":{}},{"cell_type":"code","source":"def thompson_sampling_bandit(dataset, num_arms, num_rounds):\n    # Parameters for the Beta distribution of each arm: successes (alpha) and failures (beta)\n    alpha = np.ones(num_arms)\n    beta = np.ones(num_arms)\n    \n    total_rewards = 0\n\n    # Iterate through the specified number of rounds\n    for _ in range(num_rounds):\n        # Sample from the Beta distribution for each arm\n        theta = [np.random.beta(alpha[i] + 1, beta[i] + 1) for i in range(num_arms)]\n        \n        # Choose the arm with the highest sampled probability\n        chosen_arm = np.argmax(theta)\n\n        # Randomly select an observation for the chosen arm from the dataset\n        # Here, filtering dataset where 'match_id' equals chosen_arm and sampling one row\n        chosen_row = dataset[dataset['match_id'] == chosen_arm].sample(n=1)\n        \n        # Observe the reward (response) for the chosen arm\n        reward = chosen_row['response'].values[0]\n        total_rewards += reward\n        \n        # Update the alpha and beta for the chosen arm based on the reward\n        if reward > 0:\n            alpha[chosen_arm] += 1\n        else:\n            beta[chosen_arm] += 1\n\n    return alpha, beta, total_rewards","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:32.082519Z","iopub.execute_input":"2024-04-21T20:06:32.082959Z","iopub.status.idle":"2024-04-21T20:06:32.093639Z","shell.execute_reply.started":"2024-04-21T20:06:32.082924Z","shell.execute_reply":"2024-04-21T20:06:32.091876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_arms = 100 # Number of items\nnum_rounds = 100  # Number of interactions\nresults = thompson_sampling_bandit(predf, num_arms, num_rounds)\nprint(\"Updated alpha and beta parameters:\", results[0:2])\nprint(\"Total rewards accumulated:\", results[2])","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:32.098584Z","iopub.execute_input":"2024-04-21T20:06:32.099004Z","iopub.status.idle":"2024-04-21T20:06:32.297196Z","shell.execute_reply.started":"2024-04-21T20:06:32.098972Z","shell.execute_reply":"2024-04-21T20:06:32.295902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear Greedy","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\ndef linear_greedy_bandit(dataset, num_arms, num_rounds):\n    model = LinearRegression()\n    total_rewards = 0\n    chosen_arms = []\n    \n    # Initialize the dataset for training the model\n    # Start with one example for each arm if possible\n    training_data = pd.DataFrame()\n    for arm in range(num_arms):\n        if dataset[dataset['match_id'] == arm].shape[0] > 0:\n            # Append the first instance for each arm to the training data\n            # Use concat instead of append\n            training_data = pd.concat([training_data, dataset[dataset['match_id'] == arm].iloc[:1]], ignore_index=True)\n\n    # Check if there are any entries in the training data before fitting\n    if not training_data.empty:\n        # Training the model on initial data\n        model.fit(training_data[['similarity_score']], training_data['response'])\n\n        for _ in range(num_rounds):\n            predictions = model.predict(dataset[['similarity_score']])\n            dataset['predicted_reward'] = predictions\n\n            # Choose the arm with the highest predicted reward\n            max_indices = dataset.groupby('match_id')['predicted_reward'].idxmax()\n            chosen_arm = dataset.loc[max_indices].sample(n=1)\n            chosen_arm_index = chosen_arm.index.item()\n\n            # Observe the reward\n            reward = dataset.at[chosen_arm_index, 'response']\n            total_rewards += reward\n            chosen_arms.append(chosen_arm['match_id'].values[0])\n            \n            # Add this data point to the training dataset\n            training_data = pd.concat([training_data, dataset.loc[chosen_arm_index:chosen_arm_index]], ignore_index=True)\n\n            # Re-train the model with updated data\n            model.fit(training_data[['similarity_score']], training_data['response'])\n\n    return chosen_arms, total_rewards\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:32.299028Z","iopub.execute_input":"2024-04-21T20:06:32.299523Z","iopub.status.idle":"2024-04-21T20:06:32.313134Z","shell.execute_reply.started":"2024-04-21T20:06:32.299475Z","shell.execute_reply":"2024-04-21T20:06:32.311695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## linear greedy with exploration (epsilon)","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nimport random\nimport pandas as pd\n\ndef linear_greedy_bandit_epsilon(dataset, num_arms, epsilon, num_rounds):\n    model = LinearRegression()\n    total_rewards = 0\n    chosen_arms = []\n    \n    # Initialize the dataset for training the model\n    training_data = pd.DataFrame()\n    for arm in range(num_arms):\n        # Attempt to start with one example for each arm\n        arm_data = dataset[dataset['match_id'] == arm]\n        if not arm_data.empty:\n            training_data = pd.concat([training_data, arm_data.iloc[:1]], ignore_index=True)\n\n    # Training the model on initial data if available\n    if not training_data.empty:\n        model.fit(training_data[['similarity_score']], training_data['response'])\n\n        for _ in range(num_rounds):\n            # Exploration vs. Exploitation\n            if random.random() < epsilon:\n                # Explore: Randomly select an arm\n                chosen_arm_index = dataset.sample(n=1).index.item()\n            else:\n                # Exploit: Select the arm with the highest predicted reward\n                predictions = model.predict(dataset[['similarity_score']])\n                dataset.loc[:, 'predicted_reward'] = predictions\n                max_indices = dataset.groupby('match_id')['predicted_reward'].idxmax()\n                chosen_arm_index = dataset.loc[max_indices].sample(n=1).index.item()\n            \n            # Observe the reward and the chosen arm\n            reward = dataset.at[chosen_arm_index, 'response']\n            total_rewards += reward\n            chosen_arm = dataset.at[chosen_arm_index, 'match_id']\n            chosen_arms.append(chosen_arm)\n            \n            # Add this data point to the training dataset\n            training_data = pd.concat([training_data, dataset.loc[chosen_arm_index:chosen_arm_index]], ignore_index=True)\n            \n            # Re-train the model with the updated training data\n            model.fit(training_data[['similarity_score']], training_data['response'])\n\n    return chosen_arms, total_rewards\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:32.315238Z","iopub.execute_input":"2024-04-21T20:06:32.315679Z","iopub.status.idle":"2024-04-21T20:06:32.332012Z","shell.execute_reply.started":"2024-04-21T20:06:32.315642Z","shell.execute_reply":"2024-04-21T20:06:32.330602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"linear greedy output","metadata":{}},{"cell_type":"code","source":"results = linear_greedy_bandit(predf, num_arms=8, num_rounds=100)\nprint(\"Chosen arms per round:\", results[0])\nprint(\"Total rewards accumulated:\", results[1])","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:32.333843Z","iopub.execute_input":"2024-04-21T20:06:32.334220Z","iopub.status.idle":"2024-04-21T20:06:35.424215Z","shell.execute_reply.started":"2024-04-21T20:06:32.334191Z","shell.execute_reply":"2024-04-21T20:06:35.423144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"linear greedy with epsilon","metadata":{}},{"cell_type":"code","source":"results = linear_greedy_bandit_epsilon(predf, num_arms=8,epsilon=0.5, num_rounds=100)\nprint(\"Chosen arms per round:\", results[0])\nprint(\"Total rewards accumulated:\", results[1])","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:35.425534Z","iopub.execute_input":"2024-04-21T20:06:35.425880Z","iopub.status.idle":"2024-04-21T20:06:38.469777Z","shell.execute_reply.started":"2024-04-21T20:06:35.425853Z","shell.execute_reply":"2024-04-21T20:06:38.468567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:38.471026Z","iopub.execute_input":"2024-04-21T20:06:38.471416Z","iopub.status.idle":"2024-04-21T20:06:38.477211Z","shell.execute_reply.started":"2024-04-21T20:06:38.471387Z","shell.execute_reply":"2024-04-21T20:06:38.475993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Simulation, Validation & Benchmarking","metadata":{}},{"cell_type":"markdown","source":"specific user for matchmaking","metadata":{}},{"cell_type":"code","source":"user_id = 2\nuser_data = predf[predf['user_id'] == user_id]\nnum_arms = user_data['match_id'].nunique()","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:38.478532Z","iopub.execute_input":"2024-04-21T20:06:38.478883Z","iopub.status.idle":"2024-04-21T20:06:38.489362Z","shell.execute_reply.started":"2024-04-21T20:06:38.478854Z","shell.execute_reply":"2024-04-21T20:06:38.488121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_data","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:38.491248Z","iopub.execute_input":"2024-04-21T20:06:38.491740Z","iopub.status.idle":"2024-04-21T20:06:38.507929Z","shell.execute_reply.started":"2024-04-21T20:06:38.491703Z","shell.execute_reply":"2024-04-21T20:06:38.506941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Simulation: Matchmaking (user based)","metadata":{}},{"cell_type":"markdown","source":"simulation e greedy","metadata":{}},{"cell_type":"code","source":"def epsilon_greedy_matchmaking(user_id, user_data, num_arms, epsilon, num_rounds):\n    estimated_rewards = [0] * num_arms\n    num_selected = [0] * num_arms\n    total_reward = 0\n    selected_arms = []\n\n    for _ in range(num_rounds):\n        if random.random() < epsilon:\n            # Explore\n            chosen_arm = random.choice(user_data['match_id'].unique())\n        else:\n            # Exploit\n            chosen_arm = max(range(num_arms), key=lambda arm: estimated_rewards[arm])\n\n        # Simulate getting a reward for the chosen arm from the user interactions\n        reward_row = user_data[(user_data['user_id'] == user_id) & (user_data['match_id'] == chosen_arm)].sample()\n        reward = reward_row['response'].values[0]\n        total_reward += reward\n\n        # Update the estimated rewards and number of times each arm was selected\n        arm_index = list(user_data['match_id'].unique()).index(chosen_arm)\n        num_selected[arm_index] += 1\n        estimated_rewards[arm_index] += (reward - estimated_rewards[arm_index]) / num_selected[arm_index]\n\n        # If the reward is 1, add the arm to the list of selected arms\n        if reward == 1:\n            selected_arms.append(chosen_arm)\n\n    return selected_arms, total_reward","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:38.509044Z","iopub.execute_input":"2024-04-21T20:06:38.509990Z","iopub.status.idle":"2024-04-21T20:06:38.519290Z","shell.execute_reply.started":"2024-04-21T20:06:38.509958Z","shell.execute_reply":"2024-04-21T20:06:38.518424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epsilon = 0.5  # Exploration rate\nnum_rounds = 100  # Number of rounds to simulate\n\nselected_arms, total_reward = epsilon_greedy_matchmaking(user_id, user_data, num_arms, epsilon, num_rounds)\nprint(\"Selected Arms:\", selected_arms)\nprint(\"Total Reward:\", total_reward)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:38.520320Z","iopub.execute_input":"2024-04-21T20:06:38.521294Z","iopub.status.idle":"2024-04-21T20:06:38.655926Z","shell.execute_reply.started":"2024-04-21T20:06:38.521249Z","shell.execute_reply":"2024-04-21T20:06:38.654824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"simulation thompson sampling","metadata":{}},{"cell_type":"code","source":"def thompson_sampling_matchmaking(user_id, user_data, num_arms, num_rounds):\n    alpha = np.ones(num_arms)\n    beta = np.ones(num_arms)\n    total_reward = 0\n    selected_arms = []\n\n    for _ in range(num_rounds):\n        # Sample from the Beta distribution for each arm and choose the one with the highest sample\n        chosen_arm_index = np.argmax([np.random.beta(a + 1, b + 1) for a, b in zip(alpha, beta)])\n        chosen_arm = user_data['match_id'].unique()[chosen_arm_index]\n        \n        # Simulate getting a reward for the chosen arm from the user interactions\n        reward_row = user_data[(user_data['user_id'] == user_id) & (user_data['match_id'] == chosen_arm)].sample()\n        reward = reward_row['response'].values[0]\n        total_reward += reward\n\n        # If the reward is 1, add the arm to the list of selected arms\n        if reward == 1:\n            selected_arms.append(chosen_arm)\n        \n        # Update the alpha and beta values for the chosen arm\n        alpha[chosen_arm_index] += reward\n        beta[chosen_arm_index] += (1 - reward)\n\n    return selected_arms, total_reward\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:38.657146Z","iopub.execute_input":"2024-04-21T20:06:38.657449Z","iopub.status.idle":"2024-04-21T20:06:38.667341Z","shell.execute_reply.started":"2024-04-21T20:06:38.657423Z","shell.execute_reply":"2024-04-21T20:06:38.666105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_arms, total_reward = thompson_sampling_matchmaking(user_id, user_data, num_arms, num_rounds)\nprint(\"Selected Arms:\", selected_arms)\nprint(\"Total Reward:\", total_reward)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:38.668831Z","iopub.execute_input":"2024-04-21T20:06:38.669204Z","iopub.status.idle":"2024-04-21T20:06:39.042565Z","shell.execute_reply.started":"2024-04-21T20:06:38.669175Z","shell.execute_reply":"2024-04-21T20:06:39.041387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"simulation linear greedy","metadata":{}},{"cell_type":"code","source":"def linear_greedy_epsilon_matchmaking(user_id, user_data, num_arms, epsilon, num_rounds):\n    model = LinearRegression()\n    total_reward = 0\n    selected_arms = []\n    \n    # Initialize the dataset for training the model\n    training_data = pd.DataFrame()\n    for arm in range(num_arms):\n        arm_data = user_data[user_data['match_id'] == arm]\n        if not arm_data.empty:\n            training_data = pd.concat([training_data, arm_data.iloc[:1]], ignore_index=True)\n    \n    # Check if there are any entries in the training data before fitting\n    if not training_data.empty:\n        model.fit(training_data[['similarity_score']], training_data['response'])\n\n        for _ in range(num_rounds):\n            if random.random() < epsilon:\n                # Explore\n                chosen_arm = random.choice(user_data['match_id'].unique())\n            else:\n                # Exploit: Choose the arm with the highest predicted reward\n                predictions = model.predict(user_data[['similarity_score']])\n                user_data.loc[:, 'predicted_reward'] = predictions\n                chosen_arm = user_data.loc[user_data['predicted_reward'].idxmax(), 'match_id']\n            \n            # Observe the reward and update the model accordingly\n            reward_row = user_data[(user_data['user_id'] == user_id) & (user_data['match_id'] == chosen_arm)].sample()\n            reward = reward_row['response'].values[0]\n            total_reward += reward\n\n            # If the reward is 1, add the arm to the list of selected arms\n            if reward == 1:\n                selected_arms.append(chosen_arm)\n            \n            # Add this data point to the training dataset and retrain the model\n            if chosen_arm in training_data['match_id'].values:\n                # Update the existing data point\n                arm_index = training_data.index[training_data['match_id'] == chosen_arm].tolist()[0]\n                training_data.loc[arm_index, 'response'] = reward\n            else:\n                # Add new data point\n                training_data = pd.concat([training_data, reward_row], ignore_index=True)\n            model.fit(training_data[['similarity_score']], training_data['response'])\n\n    return selected_arms, total_reward\n","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:39.044166Z","iopub.execute_input":"2024-04-21T20:06:39.044620Z","iopub.status.idle":"2024-04-21T20:06:39.059433Z","shell.execute_reply.started":"2024-04-21T20:06:39.044590Z","shell.execute_reply":"2024-04-21T20:06:39.058118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_arms, total_reward = linear_greedy_epsilon_matchmaking(user_id, user_data, num_arms, epsilon, num_rounds)\nprint(\"Selected Arms:\", selected_arms)\nprint(\"Total Reward:\", total_reward)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:10:31.636351Z","iopub.execute_input":"2024-04-21T20:10:31.637137Z","iopub.status.idle":"2024-04-21T20:11:25.290498Z","shell.execute_reply.started":"2024-04-21T20:10:31.637073Z","shell.execute_reply":"2024-04-21T20:11:25.289429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def linear_greedy_matchmaking(user_id, user_data, num_arms, num_rounds):\n    model = LinearRegression()\n    total_reward = 0\n    selected_arms = []\n    \n    # Initialize the dataset for training the model\n    training_data = pd.DataFrame()\n    for arm in range(num_arms):\n        arm_data = user_data[user_data['match_id'] == arm]\n        if not arm_data.empty:\n            # Start with one example for each arm if possible\n            training_data = pd.concat([training_data, arm_data.iloc[:1]], ignore_index=True)\n    \n    # Training the model on initial data if available\n    if not training_data.empty:\n        model.fit(training_data[['similarity_score']], training_data['response'])\n\n        for _ in range(num_rounds):\n            # Exploit: Choose the arm with the highest predicted reward\n            predictions = model.predict(user_data[['similarity_score']])\n            user_data.loc[:, 'predicted_reward'] = predictions\n            chosen_arm = user_data.loc[user_data['predicted_reward'].idxmax(), 'match_id']\n            \n            # Simulate getting a reward for the chosen arm from the user interactions\n            reward_row = user_data[(user_data['user_id'] == user_id) & (user_data['match_id'] == chosen_arm)].sample()\n            reward = reward_row['response'].values[0]\n            total_reward += reward\n\n            # If the reward is 1, add the arm to the list of selected arms\n            if reward == 1:\n                selected_arms.append(chosen_arm)\n            \n            # Update the training data and retrain the model\n            training_data = pd.concat([training_data, reward_row], ignore_index=True)\n            model.fit(training_data[['similarity_score']], training_data['response'])\n\n    return selected_arms, total_reward","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:40.305463Z","iopub.execute_input":"2024-04-21T20:06:40.305846Z","iopub.status.idle":"2024-04-21T20:06:40.319940Z","shell.execute_reply.started":"2024-04-21T20:06:40.305816Z","shell.execute_reply":"2024-04-21T20:06:40.318629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_arms, total_reward = linear_greedy_matchmaking(user_id, user_data, num_arms, num_rounds)\nprint(\"Selected Arms:\", selected_arms)\nprint(\"Total Reward:\", total_reward)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:40.322025Z","iopub.execute_input":"2024-04-21T20:06:40.322524Z","iopub.status.idle":"2024-04-21T20:06:41.688522Z","shell.execute_reply.started":"2024-04-21T20:06:40.322481Z","shell.execute_reply":"2024-04-21T20:06:41.687680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CV VALIDATION","metadata":{}},{"cell_type":"code","source":"def cross_validate_bandits(dataset, num_arms, num_rounds, num_folds=5, epsilon=0.5):\n    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n    results = {\n        'epsilon_greedy': [],\n        'thompson_sampling': [],\n        'linear_greedy': [],\n        'linear_greedy_epsilon':[]\n    }\n\n    fold_index = 0\n    for train_index, _ in kf.split(dataset):\n        train_data = dataset.iloc[train_index]\n\n        # Epsilon Greedy\n        _, _, _, _, epsilon_rewards = epsilon_greedy_bandit(train_data, num_arms, epsilon, num_rounds)\n        results['epsilon_greedy'].append((fold_index, epsilon_rewards))\n        \n        # Thompson Sampling\n        _, _, thompson_rewards = thompson_sampling_bandit(train_data, num_arms, num_rounds)\n        results['thompson_sampling'].append((fold_index, thompson_rewards))\n        \n        # Linear Greedy\n        _, linear_rewards = linear_greedy_bandit(train_data, num_arms, num_rounds)\n        results['linear_greedy'].append((fold_index, linear_rewards))\n        \n        # Linear Greedy With Exploration\n        _, linear_epsilon_rewards = linear_greedy_bandit_epsilon(train_data, num_arms,epsilon, num_rounds)\n        results['linear_greedy'].append((fold_index, linear_epsilon_rewards))\n\n        fold_index += 1\n\n    return results","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:41.689867Z","iopub.execute_input":"2024-04-21T20:06:41.690528Z","iopub.status.idle":"2024-04-21T20:06:41.700493Z","shell.execute_reply.started":"2024-04-21T20:06:41.690485Z","shell.execute_reply":"2024-04-21T20:06:41.699623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"result + plot function","metadata":{}},{"cell_type":"code","source":"\ndef analyze_results(results):\n    for key, value in results.items():\n        print(f\"{key}: Mean Reward = {np.mean(value):.2f}, Std Dev = {np.std(value):.2f}\")\n# plot functiojn\ndef plot_results(results):\n    plt.figure(figsize=(12, 8))\n    max_folds = 0  # Variable to track the maximum number of folds encountered\n\n    for policy, data in results.items():\n        if data:  # Check if data is not empty\n            folds, rewards = zip(*data)  # Unpack fold indices and rewards\n            plt.plot(folds, rewards, marker='o', label=policy)\n            max_folds = max(max_folds, max(folds) + 1)  # Update max_folds if current is greater\n\n    plt.title('Rewards by Fold for Different Bandit Policies')\n    plt.xlabel('Fold Index')\n    plt.ylabel('Total Rewards')\n    plt.xticks(range(max_folds))  # Use max_folds to set x-axis ticks\n    plt.legend()\n    plt.grid(True)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:41.701809Z","iopub.execute_input":"2024-04-21T20:06:41.702218Z","iopub.status.idle":"2024-04-21T20:06:41.716160Z","shell.execute_reply.started":"2024-04-21T20:06:41.702188Z","shell.execute_reply":"2024-04-21T20:06:41.714857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Usage\nnum_arms = len(predf['match_id'].unique())\nnum_rounds = 10000  # Adjust this as necessary based on your scenario\n\n# Assuming a mock dataset and parameters for the demonstration\n# num_arms = 15  # Example number of arms\n# num_rounds = 100  # Example number of rounds for each fold\n# data = {\n#    'match_id': np.random.randint(0, num_arms, 1000),\n#    'response': np.random.randint(0, 2, 1000),  # Binary rewards as an example\n#    'similarity_score': np.random.random(1000)  # Example feature for linear regression\n#}\n#dataset = pd.DataFrame(data)\n\n# Run cross-validation\nresults = cross_validate_bandits(predf, num_arms, num_rounds)\nanalyze_results(results)\nplot_results(results)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:06:41.721428Z","iopub.execute_input":"2024-04-21T20:06:41.721783Z","iopub.status.idle":"2024-04-21T20:10:26.094708Z","shell.execute_reply.started":"2024-04-21T20:06:41.721754Z","shell.execute_reply":"2024-04-21T20:10:26.091548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_results(results['epsilon_greedy']+results['thompson_sampling']+result['linear_greedy_epsilon'])","metadata":{"execution":{"iopub.status.busy":"2024-04-21T20:10:26.096459Z","iopub.status.idle":"2024-04-21T20:10:26.097253Z","shell.execute_reply.started":"2024-04-21T20:10:26.096987Z","shell.execute_reply":"2024-04-21T20:10:26.097007Z"},"trusted":true},"execution_count":null,"outputs":[]}]}